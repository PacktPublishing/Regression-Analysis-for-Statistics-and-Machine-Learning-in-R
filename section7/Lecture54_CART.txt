######### CART- Classification& regression Tree #######

## continuous response Y: regression tree
## RTs donot apply a global model (unlike OLS)
## trees try to partition the data space into small enough parts 
#where we can apply a simple different model on each part. 

## high non-linearity & complex relationship between dependent 
## & independent variables, a tree model will outperform a classical regression method

setwd("F:\\RegressionModelling_R\\Lectures\\section7")
library(caret)

library(rpart)

eco=read.csv("biocap.csv")

names(eco)

set.seed(10)

inTraining =createDataPartition(eco$BiocapacityT, p = .75, list = FALSE) #create a 75% data partition
training = eco[ inTraining,] #75% data for model training
testing= eco[-inTraining,] #25% for model testing


fit = rpart(BiocapacityT ~ ., data=training, control=rpart.control(minsplit=6))

printcp(fit) # display the results 
plotcp(fit) # visualize cross-validation results 
#whether the tree could be simplified by pruning 
#make use of the plotcp function

summary(fit) # detailed summary of splits


par(mfrow=c(1,2)) # two plots on one page 

rsq.rpart(fit) # visualize cross-validation results  	

# plot tree 
plot(fit, uniform=TRUE, main="Regression Tree for Biocapacity ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

### prune the tree 
### shorten the tree, which makes trees more compact 
### avoids overfitting to the training data.
pfit<- prune(fit, cp=0.016) # from cptable
## simplify the tree based on a cp identified from the graph

# plot the pruned tree 
plot(pfit, uniform=TRUE, main="Pruned Regression Tree for Mileage")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)


### How does the model generalize?
p=predict(fit, newdata =testing) #testing was the 25% original data not used in model building

head(p)

p=as.data.frame(p)

final=cbind(testing$BiocapacityT,p) #how do predicted values (p) compare with actual test data

head(final)

cor(testing$BiocapacityT,p)

library(Metrics)

rmse(testing$BiocapacityT,p)
