print(ir.pca)
plot(ir.pca, type = "l")
summary(ir.pca)
p <- ggord(ir.pca, iris$Species)
p
library(ggord)
p <- ggord(ir.pca, iris$Species)
p
res.pca <- PCA(log.x,  graph = FALSE)
get_eig(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 90))
var <- get_pca_var(res.pca)
var
head(var$contrib)
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_pca_var(res.pca, col.var = "black")
data(iris)
head(iris)
log.x<- log(iris[, 1:4])
ir.species <- iris[, 5]
ir.pca <- prcomp(log.x,
center = TRUE,
scale. = TRUE) #apply on quantitative variables
print(ir.pca)
plot(ir.pca, type = "l")
summary(ir.pca) #how much variability do PCs explain
p <- ggord(ir.pca, iris$Species)
p
res.pca <- PCA(log.x,  graph = FALSE)
get_eig(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 90))
var <- get_pca_var(res.pca)
var
head(var$contrib)
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
install.packages("rgbif")
library("rgbif")
head(name_lookup(query='Cnaemidophorus', rank="genus", return="data"))
head(name_lookup(query='Catopsilia', rank="genus", return="data"))
x=head(name_lookup(query='Catopsilia', rank="genus", return="data"))
head(x)
head(name_lookup(query='Catopsilia pomona', return = 'data'))
occ_search(scientificName = "Catopsilia pomona", limit = 20)
dan_ple=occurrencelist(sciname = 'Danaus plexippus',
coordinatestatus = TRUE, maxresults = 1000,
latlongdf = TRUE, removeZeros = TRUE)
x=occ_search(scientificName = "Catopsilia pomona")
head(x)
library(maps)
library(ggplot2)
world = map_data("world")
ggplot(world, aes(long, lat)) +
geom_polygon(aes(group = group), fill = "white",
color = "gray40", size = .2) +
geom_jitter(data = x,
aes(decimalLongitude, decimalLatitude), alpha=0.6,
size = 4, color = "red") +
opts(title = "Danaus plexippus")
Dan_chr=occurrencelist(sciname = 'Danaus chrysippus',
coordinatestatus = TRUE,
maxresults = 1000,
latlongdf = TRUE, removeZeros = TRUE)
occurrencelist(scientificname = 'Accipiter erythronemius', coordinatestatus = TRUE,
maxresults = 5)
occ_search(scientificname = 'Accipiter erythronemius', coordinatestatus = TRUE,
maxresults = 5)
occ_search(scientificName = "Ursus americanus", limit = 5)
type(x)
class(x)
occ_search(geometry=c(-125.0,38.4,-121.8,40.9), limit=20)
x=c(1,12,30,54,5)
length(x)
typeof(x) #what type is my vector
x[2] #index number
x[1] #index number
x=c(x,8) #add another value
x
length(x)
x=c(x,"cat") #character vector
x
typeof(x)
x[1:3]
x[1:4]
series <- 1:10
seq(10)
y=seq(1, 10, by = 2)
y
typeof(y)
typeof(as.integer(y)) #coerce it into a different data type
m <- matrix(1:6, nrow = 2, ncol = 3)
m
m2=matrix(y,nrow=3,ncol=2)
m2=matrix(1:6,nrow=3,ncol=2)
m2
x <- 1:4
y <- 10:13
cbind(x, y) #join of the basis of columns
nrow(cbind(x, y)) #no of rows
ncol(cbind(x, y)) #no of columns
rbind(x, y) #join by rows
df1=as.data.frame((cbind(x,y))) #convert to data farme
df1
str(df1)
length(df1)
data(package="datasets")
data(ChickWeight)
str(ChickWeight)
head(ChickWeight)
library(corrplot)
corr1<-cor(mtcars) #compute multiple correlations
corr1
corrplot(x)
corrplot(corr1, method="color")
hist(mtcars$mpg)
hist(mtcars$wt)
shapiro.test(mtcars$mpg)
shapiro.test(mtcars$wt) # => p = 0.09
head(mtcars)
df=mtcars
fit1=lm(mpg~hp,data=df)
summary(fit1)
fit2=lm(Petal.Length~.-Species, data=iris)#remove species from analysis
summary(fit2)
fit2=lm(Petal.Length~., data=iris)
summary(fit2)
plot(fit2)
library(MASS)
bc <- boxcox(Sepal.Width ~ Petal.Width, data=iris)
trans <- bc$x[which.max(bc$y)]
trans
fit4 <- lm(Sepal.Width^trans ~ Petal.Width, data=iris)
summary(fit4)
par(mfrow = c(2, 2))
plot(fit4) #not all data can be transformed
exp(-0.69)
install.packages("pdp")
x=read.csv("sha2.csv")
head(x)
install.packages("h2o")
library(h2o)
h2o.init(nthreads = -1)
h2o.shutdown(prompt = TRUE)
install.packages("devtools")
devtools::install_github("rstudio/keras")
devtools::install_github("rstudio/tensorflow")
library("LearnBayes", lib.loc="~/R/win-library/3.3")
detach("package:LearnBayes", unload=TRUE)
install.packages("LearnBayes")
library(LearnBayes)
install.packages("BLR")
install.packages("brms")
install.packages("SSDM")
library(SSDM)
gui()
Env <- load_var(system.file('extdata',  package = 'SSDM'), categorical = 'SUBSTRATE', verbose = FALSE)
Env
Occ <- load_occ(path = system.file('extdata',  package = 'SSDM'), Env,
Xcol = 'LONGITUDE', Ycol = 'LATITUDE',
file = 'Occurrences.csv', sep = ',', verbose = FALSE)
head(Occ)
SDM <- modelling('GLM', subset(Occurrences, Occurrences$SPECIES == 'elliptica'),
Env, Xcol = 'LONGITUDE', Ycol = 'LATITUDE', verbose = FALSE)
plot(SDM@projection, main = 'SDM\nfor Cryptocarya elliptica\nwith GLM algorithm')
ESDM <- ensemble_modelling(c('CTA', 'MARS'), subset(Occurrences, Occurrences$SPECIES == 'elliptica'),
Env, rep = 1, Xcol = 'LONGITUDE', Ycol = 'LATITUDE',
ensemble.thresh = c(0.6), verbose = FALSE)
plot(ESDM@projection, main = 'ESDM\nfor Cryptocarya elliptica\nwith CTA and MARS algorithms')
SSDM <- stack_modelling(c('CTA', 'SVM'), Occurrences, Env, rep = 1,
Xcol = 'LONGITUDE', Ycol = 'LATITUDE',
Spcol = 'SPECIES', method = "pSSDM", verbose = FALSE)
plot(SSDM@diversity.map, main = 'SSDM\nfor Cryptocarya genus\nwith CTA and SVM algorithms')
knitr::kable(ESDM@evaluation)
knitr::kable(SSDM@evaluation)
knitr::kable(SSDM@variable.importance)
plot(SSDM@endemism.map, main = 'Endemism map\nfor Cryptocarya genus\nwith CTA and SVM algorithms')
plot(SSDM)
library(help = "datasets")
data(HairEyeColor)
head(HairEyeColor)
attach(HairEyeColor)
data(BOD)
head(BOD)
data("ChickWeight")
head(ChickWeight)
head(Orange)
fit(Orange$circumference~Orange$age)
fit=lm(Orange$circumference~Orange$age)
summary(fit)
plot(Orange$circumference~Orange$age)
library(help = "datasets")
head(Orange)
plot(Orange$age,Orange$circumference)
fit=lm(circumference~age,data=Orange)
summary(fit)
library(help = "datasets")
summary(fit)
abline((circumference~age,data=Orange))
abline((circumference~age))
abline((Orange$circumference~Orange$age))
plot(Orange$age,Orange$circumference)
abline((Orange$circumference~Orange$age))
library(ggplot2)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
library(help = "datasets")
data("Orange")
head(Orange)
plot(Orange$age,Orange$circumference)
fit=lm(circumference~age,data=Orange)
summary(fit)
library(ggplot2)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
fit=lm(circumference~age,data=Orange)
summary(fit)
p=ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
p1 <- p + geom_text(x = 25, y = 300, label = lm_eqn(fit), parse = TRUE)
lm_eqn <- function(df){
m <- lm(y ~ x, df);
eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,
list(a = format(coef(m)[1], digits = 2),
b = format(coef(m)[2], digits = 2),
r2 = format(summary(m)$r.squared, digits = 3)))
as.character(as.expression(eq));
}
p1 <- p + geom_text(x = 25, y = 300, label = lm_eqn(fit), parse = TRUE)
fit=lm(circumference~age,data=Orange)
summary(fit)
p=
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
p1 <- p + ggtitle("OLS regression line")
p1
p2 <- p1 + annotate("text", x=0.1, y=-0.05, label = "R^2=0.89") +
annotate("text", x=0.1, y=-0.06, label = "intercept=17.4") +
annotate("text", x=0.1, y=-0.07, label = "beta=0.106")
p2
p2 <- p1 + annotate("text", x=0.1, y=-0.05, label = "R^2=0.89") +
annotate("text", x=0.1, y=-0.6, label = "intercept=17.4") +
annotate("text", x=0.1, y=-0.7, label = "beta=0.106")
p2
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
head(Orange)
age=1500
circ=0.106*age
circ
fit=lm(circumference~age,data=Orange)
summary(fit)
age=1500
circ=0.106*age
circ
fit=lm(circumference~age,data=Orange)
summary(fit)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
new.dat = data.frame(age=150)
predict(fit, newdata = new.dat, interval = 'confidence')
new.dat = data.frame(age=1500)
predict(fit, newdata = new.dat, interval = 'confidence')
confint(fit)
library(ggplot2)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
fit=lm(circumference~age,data=Orange)
summary(fit)
new.dat = data.frame(age=1500)
predict(fit, newdata = new.dat, interval = 'confidence')
confint(fit)
library(ggplot2)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
fit=lm(circumference~age,data=Orange)
summary(fit)
confint(fit)
fit0=lm(circumference~age+0,data=Orange)
summary(fit0)
confint(fit) #CI of intercept conatains a 0
fit0=lm(circumference~age+0,data=Orange)
summary(fit0)
install.packages("pls")
library(mlbench)
library(help = "mlbench")
data(BostonHousing)
summary(BostonHousing)
head(BostonHousing)
library(ggplot2)
library(car)
library(caret)
library(corrplot)
head(BostonHousing)
pcr_model <- pcr(medv~., data = BostonHousing, scale = TRUE, validation = "CV")
require(pls)
set.seed (1000)
pcr_model <- pcr(medv~., data = BostonHousing, scale = TRUE, validation = "CV")
summary(pcr_model)
validationplot(pcr_model)
validationplot(pcr_model, val.type = "R2")
predplot(pcr_model)
library(relaimpo)
library(relaimpo)
calc.relimp(fit, type = c("lmg"), rela = TRUE)
fit0=lm(medv~., data = BostonHousing)
calc.relimp(fit0, type = c("lmg"), rela = TRUE)
fit0=lm(circumference~age+0,data=Orange)
calc.relimp(fit0, type = c("lmg"), rela = TRUE)
calc.relimp(fit, type = c("lmg"), rela = TRUE)
fit1=lm(medv~., data = BostonHousing)
calc.relimp(fit1, type = c("lmg"), rela = TRUE)
require(pls)
set.seed (1000)
pcr_model <- pcr(medv~., data = BostonHousing, scale = TRUE, validation = "CV")
summary(pcr_model)
summary(pcr_model)
validationplot(pcr_model)
predplot(pcr_model)
library(caret)
myfolds <- createMultiFolds(BostonHousing$medv, k = 5, times = 10)
control <- trainControl("repeatedcv", index = myfolds, selectionFunction = "oneSE")
mod1 <- train(medv ~ ., data = BostonHousing,
method = "pls",
metric = "Accuracy",
tuneLength = 20,
trControl = control,
preProc = c("zv","center","scale"))
mod1 <- train(medv ~ ., data = BostonHousing,
method = "pls",
metric = "R2",
tuneLength = 20,
trControl = control,
preProc = c("zv","center","scale"))
plot(mod1)
ncol(BostonHousing)
install.packages("plsdepot")
library(plsdepot)
head(BostonHousing)
library(plsdepot)
pls1 = plsreg1(BostonHousing[, 1:13], BostonHousing[, 14, drop = FALSE], comps = 6)
X = BostonHousing[,1:13]
pls1 = plsreg1(X, BostonHousing$medv, comps = 6)
class(X)
bh = BostonHousing[ ,c(1:13,14)]
pls1 = plsreg1(bh[, 1:13], bh[, 14, drop = FALSE], comps = 6)
pls1 = plsreg1(bh[,1:13], bh[, 14], comps = 6)
data(vehicles)
head(vehicles)
names(vehicles)
cars = vehicles[ ,c(1:12,14:16,13)]
pls1 = plsreg1(cars[, 1:15], cars[, 16, drop = FALSE], comps = 3)
pls1 = plsreg1(cars[, 1:15], cars[, 16, drop = FALSE], comps = 5)
pls1
pls1$R2
coefficients = coef(pls1)
sum.coef = sum(sapply(coefficients, abs))
c = coef(pls1)
sum.coef = sum(sapply(c, abs))
barplot(tail(c, 5))
library(mlbench)
library(help = "mlbench")
head(BostonHousing)
library(pls)
set.seed (1000)
pcr_model <- pcr(medv~., data = BostonHousing, scale = TRUE, validation = "CV")
summary(pcr_model)
validationplot(pcr_model) #decline in error in the predicted value of Y
validationplot(pcr_model, val.type = "R2") #variation Y explained as we
predplot(pcr_model) #predicted Y vs actual Y
validationplot(pcr_model) #decline in error in the predicted value of Y
install.packages("glmnet")
install.packages("tidyverse")
install.packages("broom")
library(tidyverse)
library(broom)
library(glmnet)
head(BostonHousing)
y = BostonHousing$medv
y = BostonHousing$medv
x = BostonHousing %>% select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat) %>% data.matrix()
lambdas = 10^seq(3, -2, by = -.1)
fit <- glmnet(x, y, alpha = 0, lambda = lambdas) #alpha=0 in ridge
cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
plot(cv_fit)
optlambda <- cv_fit$lambda.min
optlambda
y_predicted <- predict(fit, s = opt_lambda, newx = x)
y_predicted <- predict(fit, s = optlambda, newx = x)
sst <- sum(y^2)
sse <- sum((y_predicted - y)^2)
rsq <- 1 - sse / sst
rsq
library(caret)
mod1 <- train(medv ~ ., data = BostonHousing,
method = "pls",
metric = "R2",
tuneLength = 20,
trControl = control,
preProc = c("zv","center","scale"))
myfolds <- createMultiFolds(BostonHousing$medv, k = 5, times = 10)
control <- trainControl("repeatedcv", index = myfolds, selectionFunction = "oneSE")
mod1 <- train(medv ~ ., data = BostonHousing,
method = "pls",
metric = "R2",
tuneLength = 20,
trControl = control,
preProc = c("zv","center","scale"))
plot(mod1) #how inclusion of designated latent variables decreses error
library(plsdepot)
data(vehicles)
head(vehicles)
cars = vehicles[ ,c(1:12,14:16,13)]
pls1 = plsreg1(cars[, 1:15], cars[, 16, drop = FALSE], comps = 5)
pls1
pls1$R2 ##R2 for each of our components
head(BostonHousing)
y = BostonHousing$medv
x = BostonHousing %>% select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat) %>% data.matrix()
lambdas = 10^seq(3, -2, by = -.1)#specify a range of lambda
fit <- glmnet(x, y, alpha = 0, lambda = lambdas) #alpha=0 in ridge
cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
plot(cv_fit)
optlambda <- cv_fit$lambda.min
optlambda
y_predicted <- predict(fit, s = optlambda, newx = x)
sst <- sum(y^2)
sse <- sum((y_predicted - y)^2)
rsq <- 1 - sse / sst
rsq
data(BostonHousing)
str(BostonHousing)
summary(BostonHousing)
head(BostonHousing)
fit <- glmnet(x, y, alpha = 1, lambda = lambdas) #alpha=0 in ridge
cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
plot(cv_fit)
optlambda <- cv_fit$lambda.min
optlambda
y_predicted <- predict(fit, s = optlambda, newx = x)
sst <- sum(y^2)
sse <- sum((y_predicted - y)^2)
rsq <- 1 - sse / sst
rsq
fitl <- lars(x, y, type="lasso")
install.packages("lars")
glass=read.csv("glassClass.csv")
setwd("F:\\RegressionModelling_R\\Data\\section6")
library(caret)
glass=read.csv("glassClass.csv")
head(glass)
glass$Type= as.factor(glass$Type)
table(glass$Type)
set.seed(99)
Train = createDataPartition(glass$Type, p=0.75, list=FALSE)
training = glass[ Train, ] #75% training
testing = glass[ -Train, ]
library(nnet)
gModel = multinom(Type~., data=training, maxit=500, trace=T)
varImp(gModel) #importance of the different predictors
p1= predict(gModel,type="class", newdata=testing)
head(p1)
p2= predict(gModel,type="probs", newdata=testing)
head(p2)
postResample(testing$Type,p1)
Model1<-lm(Ozone~Solar.R,data=airquality)
summary(Model1)
ncvTest(Model1)
library(car)
ncvTest(Model1)
Model2<-lm(Ozone~Solar.R,data=airquality,weights=(2*seq(nrow(airquality),1,-1)))
summary(Model2)
reg1 <- lm(medv ~., data =BostonHousing)
summary(reg1)
ncvTest(reg1)
distm =BoxCoxTrans(BostonHousing$medv)
print(distm)
BostonHousing=cbind(BostonHousing, m_new=predict(distm, BostonHousing$medv))
head(BostonHousing)
reg_bc=lm(m_new~.,data=BostonHousing)
ncvTest(reg_bc)
library(lmvar)
library(datasets)
help("attenu")
X = cbind(attenu$mag, attenu$dist)
colnames(X) = c("mag", "dist")
X_s = cbind(attenu$mag, 1 / attenu$dist)
colnames(X_s) = c("mag", "dist_inv")
fit_lmvar  = lmvar(attenu$accel, X, X_s)
summary(fit_lmvar)
fit_lm = lm(attenu$accel ~ mag + dist, attenu)
AIC(fit_lm)
AIC(fit_lmvar)
data(iris)
head(iris)
fit1=lm(Sepal.Length~Sepal.Width+Petal.Length,data=iris)
summary(fit1)
fit2=lm(Sepal.Length~Sepal.Width+Petal.Length+Petal.Width,data=iris)
summary(fit2)
data(iris)
head(iris)
fit1=lm(Sepal.Length~Sepal.Width+Petal.Length,data=iris)
summary(fit1)
fit2=lm(Sepal.Length~Sepal.Width+Petal.Length+Petal.Width,data=iris)
summary(fit2)
